{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP的词向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C  p1_a  p1_b  p2_a  p2_b  p2_c\n",
      "0  1     1     0     0     1     0\n",
      "1  2     0     1     1     0     0\n",
      "2  3     1     0     0     0     1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82438\\AppData\\Local\\conda\\conda\\envs\\pytorch_gpu\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>A_a</th>\n",
       "      <th>A_b</th>\n",
       "      <th>B_a</th>\n",
       "      <th>B_b</th>\n",
       "      <th>B_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  A_a  A_b  B_a  B_b  B_c\n",
       "0  a  b  1    1    0    0    1    0\n",
       "1  b  a  2    0    1    1    0    0\n",
       "2  a  c  3    1    0    0    0    1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pandas:\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'], 'C': [1, 2, 3]})\n",
    "print(pd.get_dummies(df, prefix={'A':'p1', 'B':'p2'}))\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'], 'C': [1, 2, 3]})\n",
    "one_hot=OneHotEncoder()\n",
    "data_temp=pd.DataFrame(one_hot.fit_transform(df[['A','B']]).toarray(),\n",
    "             columns=one_hot.get_feature_names(['A','B']), dtype='int32')\n",
    "data_onehot=pd.concat((df,data_temp),axis=1)\n",
    "\n",
    "data_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "  (0, 8)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus of text documents\n",
    "corpus = [\n",
    "    \"This is the first document. this.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a BOW representation\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary and BOW representation of the first document\n",
    "print(vectorizer.vocabulary_)\n",
    "print(bow[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82438\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.807 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'这是': 5, '第一篇': 1, '文章': 0, '第二篇': 3, '第三篇': 2, '第四篇': 4}\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# 如果是中文预料可以用jieba分词\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus of Chinese text documents\n",
    "corpus = [\n",
    "    \"这是第一篇文章。\",\n",
    "    \"这是第二篇文章。\",\n",
    "    \"这是第三篇文章。\",\n",
    "    \"这是第四篇文章。\",\n",
    "]\n",
    "\n",
    "# Tokenize the text data using jieba\n",
    "corpus_tokenized = [' '.join(jieba.cut(doc)) for doc in corpus]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a BOW representation\n",
    "bow = vectorizer.fit_transform(corpus_tokenized)\n",
    "\n",
    "# Print the vocabulary and BOW representation of the first document\n",
    "print(vectorizer.vocabulary_)\n",
    "print(bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 8)\t0.38408524091481483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus of text documents\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a TF-IDF representation\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary and TF-IDF representation of the first document\n",
    "print(vectorizer.vocabulary_)\n",
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### words embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82438\\AppData\\Local\\conda\\conda\\envs\\pytorch_gpu\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import fasttext\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "import logging\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\82438\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.838 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentance = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "## 对句子进行分词分词\n",
    "def segment_sen(sen):\n",
    "    sen_list = []\n",
    "    try:\n",
    "        sen_list = jieba.lcut(sen)\n",
    "    except:\n",
    "            pass\n",
    "    return sen_list   \n",
    "# 将数据变成gensim中 word2wec函数的数据格式\n",
    "sens_list = [segment_sen(i) for i in sentance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 14:56:06,884 : INFO : collecting all words and their counts\n",
      "2023-08-08 14:56:06,885 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-08-08 14:56:06,886 : INFO : collected 14 word types from a corpus of 42 raw words and 4 sentences\n",
      "2023-08-08 14:56:06,887 : INFO : Loading a fresh vocabulary\n",
      "2023-08-08 14:56:06,888 : INFO : effective_min_count=1 retains 14 unique words (100% of original 14, drops 0)\n",
      "2023-08-08 14:56:06,888 : INFO : effective_min_count=1 leaves 42 word corpus (100% of original 42, drops 0)\n",
      "2023-08-08 14:56:06,890 : INFO : deleting the raw counts dictionary of 14 items\n",
      "2023-08-08 14:56:06,891 : INFO : sample=0.001 downsamples 14 most-common words\n",
      "2023-08-08 14:56:06,892 : INFO : downsampling leaves estimated 5 word corpus (11.9% of prior 42)\n",
      "2023-08-08 14:56:06,893 : INFO : estimated required memory for 14 words and 100 dimensions: 18200 bytes\n",
      "2023-08-08 14:56:06,894 : INFO : resetting layer weights\n",
      "2023-08-08 14:56:06,900 : INFO : training model with 3 workers on 14 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2023-08-08 14:56:06,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,907 : INFO : EPOCH - 1 : training on 42 raw words (6 effective words) took 0.0s, 1664 effective words/s\n",
      "2023-08-08 14:56:06,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,913 : INFO : EPOCH - 2 : training on 42 raw words (4 effective words) took 0.0s, 1225 effective words/s\n",
      "2023-08-08 14:56:06,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,921 : INFO : EPOCH - 3 : training on 42 raw words (2 effective words) took 0.0s, 561 effective words/s\n",
      "2023-08-08 14:56:06,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,934 : INFO : EPOCH - 4 : training on 42 raw words (8 effective words) took 0.0s, 1992 effective words/s\n",
      "2023-08-08 14:56:06,941 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,942 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,943 : INFO : EPOCH - 5 : training on 42 raw words (8 effective words) took 0.0s, 2181 effective words/s\n",
      "2023-08-08 14:56:06,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:06,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:06,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:06,952 : INFO : EPOCH - 6 : training on 42 raw words (5 effective words) took 0.0s, 1094 effective words/s\n",
      "2023-08-08 14:56:07,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,031 : INFO : EPOCH - 7 : training on 42 raw words (4 effective words) took 0.0s, 1191 effective words/s\n",
      "2023-08-08 14:56:07,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,050 : INFO : EPOCH - 8 : training on 42 raw words (4 effective words) took 0.0s, 1121 effective words/s\n",
      "2023-08-08 14:56:07,056 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,062 : INFO : EPOCH - 9 : training on 42 raw words (6 effective words) took 0.0s, 810 effective words/s\n",
      "2023-08-08 14:56:07,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,081 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,081 : INFO : EPOCH - 10 : training on 42 raw words (5 effective words) took 0.0s, 1463 effective words/s\n",
      "2023-08-08 14:56:07,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,090 : INFO : EPOCH - 11 : training on 42 raw words (6 effective words) took 0.0s, 1581 effective words/s\n",
      "2023-08-08 14:56:07,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,097 : INFO : EPOCH - 12 : training on 42 raw words (5 effective words) took 0.0s, 1875 effective words/s\n",
      "2023-08-08 14:56:07,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,104 : INFO : EPOCH - 13 : training on 42 raw words (5 effective words) took 0.0s, 1436 effective words/s\n",
      "2023-08-08 14:56:07,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,114 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,115 : INFO : EPOCH - 14 : training on 42 raw words (8 effective words) took 0.0s, 1698 effective words/s\n",
      "2023-08-08 14:56:07,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,122 : INFO : EPOCH - 15 : training on 42 raw words (4 effective words) took 0.0s, 1286 effective words/s\n",
      "2023-08-08 14:56:07,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,131 : INFO : EPOCH - 16 : training on 42 raw words (4 effective words) took 0.0s, 965 effective words/s\n",
      "2023-08-08 14:56:07,134 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,137 : INFO : EPOCH - 17 : training on 42 raw words (4 effective words) took 0.0s, 1019 effective words/s\n",
      "2023-08-08 14:56:07,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,150 : INFO : EPOCH - 18 : training on 42 raw words (4 effective words) took 0.0s, 1019 effective words/s\n",
      "2023-08-08 14:56:07,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,158 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,160 : INFO : EPOCH - 19 : training on 42 raw words (6 effective words) took 0.0s, 1321 effective words/s\n",
      "2023-08-08 14:56:07,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-08-08 14:56:07,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-08-08 14:56:07,171 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-08-08 14:56:07,172 : INFO : EPOCH - 20 : training on 42 raw words (7 effective words) took 0.0s, 1319 effective words/s\n",
      "2023-08-08 14:56:07,173 : INFO : training on a 840 raw words (105 effective words) took 0.3s, 385 effective words/s\n",
      "2023-08-08 14:56:07,174 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2023-08-08 14:56:07,175 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2023-08-08 14:56:07,177 : INFO : not storing attribute vectors_norm\n",
      "2023-08-08 14:56:07,178 : INFO : not storing attribute cum_table\n",
      "2023-08-08 14:56:07,181 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# sg=0:CBOW, 1:Skip_gram\n",
    "model = word2vec.Word2Vec(sens_list,min_count=1,iter=20, sg=0)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define the CNN architecture\n",
    "class TeCxCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TeCxCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_sizes[0])\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_sizes[1])\n",
    "        self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_sizes[2])\n",
    "        self.pool = nn.MaxPool1d(kernel_size=max_seq_len - max(filter_sizes) + 1)\n",
    "        self.fc1 = nn.Linear(in_features=num_filters*len(filter_sizes), out_features=hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = nn.functional.relu(self.conv1(x))\n",
    "        x2 = nn.functional.relu(self.conv2(x))\n",
    "        x3 = nn.functional.relu(self.conv3(x))\n",
    "        x1 = self.pool(x1)\n",
    "        x2 = self.pool(x2)\n",
    "        x3 = self.pool(x3)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = x.view(-1, self.num_filters*len(self.filter_sizes))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the hyperparameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "max_seq_len = 100\n",
    "hidden_dim = 100\n",
    "dropout_prob = 0.5\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the dataset and preprocess the text data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Split the dataset into the text data and label data\n",
    "texts = df['text'].values.astype(str)\n",
    "labels = df['label'].values.astype(str)\n",
    "\n",
    "# Convert the label data into numerical format\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)\n",
    "\n",
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = Tokenizer(num_words=10000, lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "max_seq_len = 100\n",
    "data = pad_sequences(sequences, maxlen=max_seq_len)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data and labels into PyTorch tensors\n",
    "train_data = torch.from_numpy(train_data).long()\n",
    "val_data = torch.from_numpy(val_data).long()\n",
    "test_data = torch.from_numpy(test_data).long()\n",
    "train_labels = torch.from_numpy(train_labels).long()\n",
    "val_labels = torch.from_numpy(val_labels).long()\n",
    "test_labels = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# Create PyTorch DataLoader objects for the training, validation, and test sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN model and define the loss function and optimizer\n",
    "model = TeCxCNN(num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the CNN on the training set and validate the model on the validation set\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Validation Accuracy: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the text\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert the tokens to embeddings\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0]\n",
    "\n",
    "# Average the embeddings\n",
    "embedding = torch.mean(embeddings, dim=1)\n",
    "\n",
    "# Use the embedding for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load the task-specific dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "X = data['text'].values.astype(str)\n",
    "y = data['label'].values.astype(int)\n",
    "encoded_labels = torch.tensor(y)\n",
    "tokenized_texts = [tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt') for text in X]\n",
    "input_ids = [tokenized_text['input_ids'][0] for tokenized_text in tokenized_texts]\n",
    "attention_masks = [tokenized_text['attention_mask'][0] for tokenized_text in tokenized_texts]\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, encoded_labels, test_size=0.2, random_state=42)\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.stack(train_inputs), torch.stack(train_masks), train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.stack(val_inputs), torch.stack(val_masks), val_labels)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "test_inputs = torch.stack(tokenizer(X_test, padding=True, truncation=True, max_length=128, return_tensors='pt')['input_ids'])\n",
    "test_masks = torch.stack(tokenizer(X_test, padding=True, truncation=True, max_length=128, return_tensors='pt')['attention_mask'])\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs, test_masks)\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# Use the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(\"This is a test sentence.\", padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = output.logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
